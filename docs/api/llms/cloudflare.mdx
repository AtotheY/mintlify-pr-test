---
title: Cloudflare AI LLM Integration
description: Learn how to integrate Cloudflare's AI with your application using the SpinAI package.
---

import CodeBlock from '@theme/CodeBlock';

# Cloudflare AI LLM Integration

This documentation covers the integration of Cloudflare's AI models into your application using the SpinAI package. The `createCloudflareAILLM` function allows you to easily configure and utilize Cloudflare's Large Language Models (LLMs) for generating text-based AI completions.

## Installation

Before you can use the Cloudflare AI integration, ensure you have the SpinAI package installed in your project. If not, you can install it using npm or yarn:

```bash
npm install @your-org/spinai
# or
yarn add @your-org/spinai
```

## Configuration

To use Cloudflare AI LLM, you need to configure it with your Cloudflare API token and account ID. Optionally, you can specify the model you wish to use; if not provided, a default model will be used.

### CloudflareConfig

The `CloudflareConfig` interface is used to configure your Cloudflare AI LLM integration.

- `apiToken`: Your Cloudflare API token.
- `accountId`: Your Cloudflare account ID.
- `model`: (Optional) The specific Cloudflare AI model you wish to use. Defaults to `@cf/meta/llama-2-7b-chat-int8`.

## Usage

To create a new Cloudflare AI LLM instance, use the `createCloudflareAILLM` function with your configuration.

### createCloudflareAILLM

This function initializes and returns a Cloudflare AI LLM instance.

<CodeBlock language="typescript">
{`import { createCloudflareAILLM, CloudflareConfig } from '@your-org/spinai';

const config: CloudflareConfig = {
  apiToken: 'your_cloudflare_api_token',
  accountId: 'your_cloudflare_account_id',
  model: '@cf/meta/llama-2-7b-chat-int8', // Optional
};

const cloudflareAILLM = createCloudflareAILLM(config);`}
</CodeBlock>

### Completing Prompts

To generate text completions, use the `complete` method provided by the Cloudflare AI LLM instance.

<CodeBlock language="typescript">
{`const completionOptions = {
  prompt: 'The meaning of life is',
  maxTokens: 512,
  temperature: 0.5,
};

cloudflareAILLM.complete(completionOptions)
  .then(completionResult => {
    console.log(completionResult.content);
  })
  .catch(error => {
    console.error('Completion error:', error);
  });`}
</CodeBlock>

#### CompletionOptions

- `prompt`: The input text prompt for the AI.
- `schema`: (Optional) A JSON schema to structure the AI's response.
- `temperature`: (Optional) Controls the randomness of the output. Lower values make the AI more deterministic.
- `maxTokens`: (Optional) The maximum number of tokens to generate. Defaults to 1024.

#### CompletionResult

- `content`: The generated text or structured content based on the provided schema.
- `inputTokens`: The estimated number of tokens used in the input prompt.
- `outputTokens`: The estimated number of tokens generated by the AI.
- `costCents`: The estimated cost of the operation in cents.
- `rawInput`: The original input prompt.
- `rawOutput`: The raw output from the AI before any parsing or processing.

## Error Handling

If the Cloudflare API token or account ID is not provided, or if the Cloudflare AI request fails, an error will be thrown. Ensure you handle these errors appropriately in your application.

## Conclusion

Integrating Cloudflare's AI LLM into your application is straightforward with the SpinAI package. By configuring your API token and account ID, you can start generating AI-powered text completions to enhance your application's capabilities.