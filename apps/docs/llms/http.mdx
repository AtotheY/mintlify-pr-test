```mdx
---
title: "HTTP LLM Integration"
description: "Integrating custom HTTP-based language models with SpinAI"
---

## Overview

The HTTP LLM integration allows you to connect any custom HTTP-based language model to the SpinAI ecosystem. This feature is designed for developers who have their own language models hosted on HTTP endpoints and wish to utilize them within SpinAI's framework for various tasks. By configuring the HTTP LLM, you can seamlessly integrate your custom models, enabling them to perform completions, generate responses based on schemas, and more.

## Usage

To use your HTTP-based language model with SpinAI, you need to create an instance of the model using the `createHttpLLM` function. This function requires a configuration object specifying details about your HTTP endpoint, optional API key, custom headers, and functions to transform requests and responses.

```typescript
import { createHttpLLM } from "spinai";

const httpLLM = createHttpLLM({
  endpoint: "https://your-model-endpoint.com",
  apiKey: "your_api_key", // Optional
  headers: { "Custom-Header": "value" }, // Optional
  transformRequest: (body) => {
    // Modify the request body before sending
    return body;
  },
  transformResponse: (response) => {
    // Process the response to extract the text
    return response.text;
  },
});
```

After creating the HTTP LLM instance, you can use it to perform completions or integrate it with other SpinAI functionalities.

## Configuration

The `createHttpLLM` function accepts a configuration object with the following properties:

```typescript
interface HttpLLMConfig {
  endpoint: string; // The URL of the HTTP endpoint for your language model
  apiKey?: string; // Optional API key for authentication
  headers?: Record<string, string>; // Optional custom headers for the HTTP request
  transformRequest?: (body: unknown) => unknown; // Optional function to transform the request body before sending
  transformResponse?: (response: unknown) => string; // Optional function to process the response and extract the desired text
}
```

## Completing Requests

The HTTP LLM instance can be used to perform completions by calling the `complete` method with a set of options:

```typescript
const completionOptions = {
  prompt: "Write a brief introduction to HTTP LLMs.",
  schema: null, // Optional JSON schema for structured responses
  temperature: 0.5, // Optional temperature setting for response generation
  maxTokens: 512, // Optional maximum number of tokens in the response
};

httpLLM.complete(completionOptions).then((result) => {
  console.log(result.content); // The generated text or structured response
});
```

The completion result includes the content generated by your model, estimated input and output tokens, the cost in cents (calculated based on token estimates), and the raw input and output.

This integration opens up a wide range of possibilities for leveraging custom language models within the SpinAI platform, providing flexibility and control over the language generation process.
```