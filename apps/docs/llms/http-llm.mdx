---
title: "Custom HTTP LLM Integration"
description: "Integrating custom HTTP-based Language Learning Models with SpinAI"
---

## Overview

The SpinAI platform now supports integrating custom HTTP-based Language Learning Models (LLMs) into your applications. This feature allows developers to leverage external LLMs by making HTTP requests, providing a flexible way to utilize various AI models with custom configurations and endpoints.

## Setting Up

To set up a custom HTTP LLM, you need to configure it with the necessary endpoint, optional API key, headers, and request/response transformations. The `createHttpLLM` function facilitates this setup.

```typescript
import { createHttpLLM } from "spinai/src/llms/http";

const httpLLM = createHttpLLM({
  endpoint: "https://your.custom.llm/endpoint",
  apiKey: "your_api_key", // Optional
  headers: { "Custom-Header": "Value" }, // Optional
  transformRequest: (body) => {
    // Modify the request body as needed
    return body;
  },
  transformResponse: (response) => {
    // Process the response to extract the text
    return response.text;
  },
});
```

## Configuration

The `HttpLLMConfig` interface outlines the configuration options for setting up your custom HTTP LLM.

```typescript
interface HttpLLMConfig {
  endpoint: string;
  apiKey?: string;
  headers?: Record<string, string>;
  transformRequest?: (body: unknown) => unknown;
  transformResponse?: (response: unknown) => string;
}
```

- `endpoint`: The URL of the custom LLM endpoint.
- `apiKey`: An optional API key for authentication.
- `headers`: Optional custom headers to include in the request.
- `transformRequest`: An optional function to transform the request body before sending.
- `transformResponse`: An optional function to transform the response into the expected format.

## Example Integrations

Below is an example of how to integrate a custom HTTP LLM into an application.

```typescript
import { createHttpLLM } from "spinai/src/llms/http";

const llm = createHttpLLM({
  endpoint: "https://api.example.com/generate-text",
  apiKey: process.env.EXAMPLE_API_KEY,
  transformResponse: (response) => response.generatedText,
});

async function generateText(prompt: string) {
  const completion = await llm.complete({
    prompt: prompt,
    maxTokens: 512,
  });

  console.log(completion.content); // Output generated by the LLM
}
```

## Token Cost Considerations

The custom HTTP LLM integration estimates the cost of tokens based on input and output character lengths, as custom endpoints might not provide explicit token counts. This estimation helps in tracking usage and understanding the potential costs associated with using the custom LLM.

```typescript
const estimatedInputTokens = Math.ceil(inputChars / 4);
const estimatedOutputTokens = Math.ceil(outputChars / 4);
```

The cost is calculated based on these estimated token counts, providing a rough guide to the expense of each request.

## Error Handling

It's important to handle potential errors, such as network issues or errors returned by the LLM endpoint. The `createHttpLLM` function throws an error if the HTTP request fails, which should be caught and handled appropriately in your application.

```typescript
try {
  const completion = await llm.complete({ prompt: "Hello, world!", maxTokens: 1024 });
  console.log(completion.content);
} catch (error) {
  console.error("Failed to generate text:", error);
}
```

By integrating custom HTTP LLMs with SpinAI, developers can extend the capabilities of their applications with various AI models, ensuring flexibility and customization to meet specific requirements.