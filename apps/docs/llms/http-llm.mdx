---
title: "Custom HTTP LLM Integration"
description: "Learn how to integrate custom HTTP-based Language Learning Models with SpinAI."
---

## Introduction

Integrating custom HTTP-based Language Learning Models (LLMs) allows for flexible and powerful extensions to the SpinAI platform. This feature enables the use of any HTTP-accessible LLM, providing a way to leverage specialized models or proprietary systems for specific tasks or domains.

## Creating a Custom HTTP LLM

To create a custom HTTP LLM, you need to configure an HTTP endpoint that your model will communicate with. This involves specifying the endpoint URL, optional API keys for authentication, and any necessary headers. Additionally, you can define transformations for the request and response to adapt the model's input and output to your needs.

```typescript
import { createHttpLLM } from "spinai";

const httpLLM = createHttpLLM({
  endpoint: "https://your-model-endpoint.com",
  apiKey: "your_api_key_here", // Optional
  headers: { "Custom-Header": "value" }, // Optional
  transformRequest: (body) => {
    // Modify the request body before sending
    return body;
  },
  transformResponse: (response) => {
    // Process the response to extract the desired output
    return response;
  },
});
```

## Configuration

The `HttpLLMConfig` interface outlines the configuration options for setting up a custom HTTP LLM. These options include the endpoint URL, API key, custom headers, and functions to transform the request and response.

```typescript
interface HttpLLMConfig {
  endpoint: string;
  apiKey?: string;
  headers?: Record<string, string>;
  transformRequest?: (body: unknown) => unknown;
  transformResponse?: (response: unknown) => string;
}
```

- `endpoint`: The URL of the HTTP endpoint for the LLM.
- `apiKey`: An optional API key for authentication with the endpoint.
- `headers`: Optional custom headers to include in the request.
- `transformRequest`: An optional function to modify the request body before sending.
- `transformResponse`: An optional function to process the response and extract the desired output.

## Cost Estimation

The custom HTTP LLM integration includes a mechanism to estimate the cost of requests based on the number of tokens processed. This estimation is crucial for managing usage and understanding the potential costs associated with using the model.

The cost is estimated based on the input and output character lengths, converted to an approximate number of tokens. This estimation allows for a rough calculation of usage costs without requiring explicit token counts from the model endpoint.

```typescript
const estimatedCost = httpLLM.complete({
  prompt: "Your prompt here",
  maxTokens: 1024, // Optional
}).then(result => {
  console.log(`Estimated cost: ${result.costCents} cents`);
});
```

This documentation covers the basics of integrating custom HTTP LLMs with SpinAI. For advanced configurations and troubleshooting, refer to the [LLM Overview](./overview.mdx) documentation.