---
title: "Integrating Cloudflare with SpinAI"
description: "Learn how to configure and use Cloudflare's LLM with SpinAI for enhanced AI capabilities."
---

## Introduction

Integrating Cloudflare's Large Language Models (LLM) with SpinAI allows developers to leverage Cloudflare's AI capabilities within their applications. This documentation covers the steps required to configure and use Cloudflare LLM with SpinAI, including setting up the necessary credentials, initializing the LLM, and estimating costs.

## Configuration

Before you can use Cloudflare's LLM with SpinAI, you need to configure it with your Cloudflare API token and account ID. Optionally, you can specify a model; if not provided, a default model is used.

```typescript
interface CloudflareConfig {
  apiToken: string;
  accountId: string;
  model?: string; // Optional, defaults to "@cf/meta/llama-2-7b-chat-int8"
}
```

Ensure you have the `CLOUDFLARE_API_TOKEN` and `CLOUDFLARE_ACCOUNT_ID` set in your environment variables or passed directly to the configuration.

## Usage Examples

### Basic Cloudflare LLM Setup

This example demonstrates how to create a Cloudflare LLM instance and use it with SpinAI. The default model and other optional parameters are used if not specified.

```typescript
import { createAgent } from "spinai";
import { createCloudflareAILLM } from "spinai/src/llms/cloudflare";

const cloudflareConfig = {
  apiToken: process.env.CLOUDFLARE_API_TOKEN,
  accountId: process.env.CLOUDFLARE_ACCOUNT_ID,
};

const llm = createCloudflareAILLM(cloudflareConfig);

const agent = createAgent({
  instructions: "Provide an overview of Cloudflare's LLM capabilities",
  actions: [], // Define your actions here
  llm,
});
```

### Advanced Configuration Options

You can specify a different model and adjust parameters like temperature and max tokens for the completion request.

```typescript
const cloudflareConfig = {
  apiToken: process.env.CLOUDFLARE_API_TOKEN,
  accountId: process.env.CLOUDFLARE_ACCOUNT_ID,
  model: "@cf/custom-model", // Custom model identifier
};

const llm = createCloudflareAILLM(cloudflareConfig);

// Use the llm in your agent as shown in the basic setup example
```

## Cost Estimation

Cloudflare AI doesn't provide token counts directly, so the cost is estimated based on character count. The estimation divides the number of characters by 4 to approximate the number of tokens used. This is important for managing and anticipating usage costs.

```typescript
// Estimated tokens calculation example
const inputChars = 100; // Number of characters in the input
const outputChars = 250; // Number of characters in the AI's response

const estimatedInputTokens = Math.ceil(inputChars / 4);
const estimatedOutputTokens = Math.ceil(outputChars / 4);

// Use these estimates to calculate costs or manage usage
```

Remember to monitor your usage and adjust your configurations as needed to optimize costs and performance.